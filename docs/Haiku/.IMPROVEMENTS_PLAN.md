# Plan de Mejoras Ollash - Sistema de Co-Working Inteligente

## ğŸ¯ VisiÃ³n General
Transformar Ollash en una plataforma de co-working IA donde el agente no solo procesa informaciÃ³n aislada, sino que **conecta puntos entre mÃºltiples fuentes**, aprende patrones de preferencia, y refina continuamente sus respuestas basÃ¡ndose en retroalimentaciÃ³n iterativa.

---

## ğŸ“‹ Fases de ImplementaciÃ³n

### FASE 1: SÃNTESIS MULTIDOCUMENTO Y CROSS-REFERENCING
**Objetivo**: Permitir anÃ¡lisis transversal entre mÃºltiples fuentes de documentaciÃ³n y configuraciÃ³n.

#### Componentes a crear:
1. **CrossReferenceAnalyzer** (`src/utils/core/cross_reference_analyzer.py`)
   - Comparar 2+ documentos y extraer similitudes/diferencias
   - AnÃ¡lisis de "gaps" entre documentaciÃ³n teÃ³rica y configuraciÃ³n real
   - Buscar inconsistencias de terminologÃ­a entre archivos

2. **KnowledgeGraphBuilder** (extender `documentation_manager.py`)
   - Mapeo de conceptos entre documentos
   - Grafo de relaciones: TÃ©rmino â†’ Documentos â†’ Secciones
   - Ãndice temÃ¡tico dinÃ¡mico

#### MÃ©todos principales:
```
CrossReferenceAnalyzer:
  - compare_documents(doc1, doc2) â†’ CommonPatterns, Differences
  - find_cross_references(term, source_dirs) â†’ List[Reference]
  - extract_inconsistencies(docs) â†’ List[Inconsistency]
  - generate_knowledge_graph(documents) â†’ Graph

KnowledgeGraphBuilder:
  - build_from_documentation() â†’ KnowledgeGraph
  - add_relationship(term1, term2, relationship_type)
  - get_concept_connections(term) â†’ List[Connection]
```

---

### FASE 2: PANEL DE ARTEFACTOS INTERACTIVOS
**Objetivo**: Transformar la UI actual en un sistema dual chat + canvas dinÃ¡mico.

#### Cambios en HTML/JS:
1. **RediseÃ±o de index.html**
   - `<div id="chat-area">`: Solo instrucciones y diÃ¡logo
   - `<div id="artifacts-panel">`: Lienzo dinÃ¡mico para artefactos
   - SeparaciÃ³n CSS clara (50/50 o flexible)

2. **Sistema de tipos de artefactos**
   ```
   ArtifactManager:
     - render_report(title, sections) â†’ HTML
     - render_diagram(mermaid_code) â†’ SVG
     - render_checklist(items, callbacks) â†’ Interactive HTML
     - render_code_artifact(language, code) â†’ Syntax-highlighted
   ```

3. **Artefactos especÃ­ficos**:
   - **Informes**: ResÃºmenes ejecutivos con mÃ©tricas
   - **Diagramas**: Mermaid.js (arquitectura, flujos, dependencias)
   - **Checklists**: Items interactivos con estado persistente
   - **Comparativas**: Tablas dinÃ¡micas cross-doc

#### Backend (blueprints):
- Nueva ruta: `/api/artifacts/render` (POST)
- Payload: `{type, content, metadata}`
- Respuesta: HTML + CSS para inyectar en panel

---

### FASE 3: MEMORIA DE PROYECTOS Y APRENDIZAJE CONTINUO
**Objetivo**: Que el agente recuerde decisiones pasadas y personalice su comportamiento.

#### Componentes a crear:
1. **DecisionContextManager** (`src/utils/core/decision_context_manager.py`)
   - Guardar: `{timestamp, decision, reasoning, project, outcome}`
   - Buscar decisiones anteriores similares
   - Sugerir patrones basados en historial

2. **Extender PreferenceManager**:
   - `output_format` (bullets vs narrative)
   - `detail_level` (executive vs technical)
   - `communication_style` (formal vs casual)
   - `preferred_diagrams` (Mermaid vs ASCII vs tables)

3. **ProjectMemory** (extender `.agent_memory.json`)
   ```json
   {
     "project_contexts": {
       "project_name": {
         "decisions": [...],
         "patterns": {...},
         "lessons_learned": [...]
       }
     },
     "preferences": {...},
     "session_history": [...]
   }
   ```

#### MÃ©todos:
```
DecisionContextManager:
  - record_decision(decision, reasoning, context)
  - find_similar_decisions(current_context) â†’ List[Decision]
  - suggest_based_on_history(query) â†’ List[Suggestion]
  
ProjectMemory:
  - get_project_context(project_name) â†’ Context
  - update_lesson_learned(project, lesson)
  - predict_user_preference(question_type) â†’ Style
```

---

### FASE 4: CICLOS DE CRÃTICA Y REFINAMIENTO (CO-EDICIÃ“N)
**Objetivo**: Feedback iterativo del usuario â†’ Mejora automÃ¡tica del agente.

#### Componentes:
1. **FeedbackRefinement** (`src/utils/core/feedback_refinement.py`)
   - User selecciona texto en artefacto
   - EnvÃ­a crÃ­tica â†’ Backend procesa
   - Agente reescribe explicando el cambio

2. **ValidaciÃ³n de Datos** (extender `llm_response_parser.py`)
   - Extraer "hechos" del resumen (parsing)
   - Verificar contra fuentes originales
   - Marcar hallazgos con confianza (high/medium/low)

3. **FileRefinerIntegration**
   - Aprovechar lÃ³gica existente
   - Agregar mÃ©todo `validate_against_sources()`

#### Flujo:
```
User selects paragraph â†’ Send critique â†’ FeedbackRefinement.process()
  â†’ Extract facts â†’ Validate â†’ Regenerate with improvements
  â†’ Store feedback â†’ Update learned patterns
```

---

### FASE 5: INGESTA MULTI-MODAL Y OCR
**Objetivo**: Que el agente pueda procesar PDFs, imÃ¡genes, y notas de voz.

#### Componentes:
1. **OCRProcessor** (extender `multi_format_ingester.py`)
   - Usar `deepseek-ocr:3b` en Ollama para OCR
   - Procesar imÃ¡genes de diagramas â†’ Texto estructurado
   - Mantener metadata de coordenadas/contexto

2. **SpeechToText Integration** (en web frontend)
   - Usar Web Speech API (navegador)
   - Dictar notas â†’ Agente resume + extrae tasks
   - Guardar en `tasks.json` automÃ¡ticamente

3. **MultiModalProcessor**
   ```
   OCRProcessor:
     - process_image(image_path) â†’ {text, metadata}
     - extract_diagram_structure(image) â†’ StructuredData
   
   SpeechProcessor:
     - transcribe_audio(blob) â†’ text
     - extract_tasks_from_transcription(text) â†’ Task[]
     - save_to_tasks_json(tasks)
   ```

---

## ğŸ› ï¸ EjecuciÃ³n TÃ©cnica

### Archivo de ConfiguraciÃ³n (`settings.json`)
Agregar secciÃ³n:
```json
{
  "features": {
    "cross_reference": true,
    "artifacts_panel": true,
    "decision_memory": true,
    "feedback_refinement": true,
    "multimodal_ingestion": true,
    "ocr_enabled": true,
    "speech_enabled": true
  },
  "ocr": {
    "model": "deepseek-ocr:3b",
    "confidence_threshold": 0.7
  },
  "artifacts": {
    "max_diagram_size": "1000x800",
    "supported_types": ["report", "diagram", "checklist", "code", "comparison"]
  }
}
```

### Base de Datos / Persistencia
- `.agent_memory.json`: Decisiones y contextos
- `.ollash_preferences.json`: Estilos y formatos (ya existe)
- `knowledge_workspace/graphs/`: Grafos de conocimiento (ChromaDB)
- `logs/feedback_history.jsonl`: Feedback del usuario para anÃ¡lisis

---

## ğŸ“Š Dependencias Externas

### Existentes (usa):
- âœ… ChromaDB (ya en uso)
- âœ… Ollama (ya en uso)
- âœ… Flask (ya en uso)
- âœ… MultiFormatIngester (ya existe)

### Nuevo:
- ğŸ†• `deepseek-ocr:3b` (modelo Ollama para OCR)
- ğŸ†• `networkx` (para grafo de conocimiento)
- ğŸ†• `mermaid.js` (CDN para diagramas interactivos)
- ğŸ†• Web Speech API (nativa en navegadores modernos)

---

## ğŸ’¾ Archivos a Crear

### Backend (`src/utils/core/`):
1. `cross_reference_analyzer.py` (250-400 lÃ­neas)
2. `knowledge_graph_builder.py` (200-300 lÃ­neas)
3. `decision_context_manager.py` (250-350 lÃ­neas)
4. `feedback_refinement.py` (200-300 lÃ­neas)
5. Extender: `documentation_manager.py`, `multi_format_ingester.py`, `preference_manager.py`

### Frontend (`src/web/`):
1. `static/js/artifacts_manager.js` (300-400 lÃ­neas)
2. `static/js/speech_processor.js` (150-200 lÃ­neas)
3. `static/css/artifacts.css` (200-300 lÃ­neas)
4. `blueprints/artifacts_bp.py` (150-200 lÃ­neas)

### ConfiguraciÃ³n:
1. Actualizar `settings.json`
2. Actualizar `index.html` (refactorizar layout)

---

## ğŸš€ Orden Recomendado de ImplementaciÃ³n

1. **Semana 1**: Fase 1 (Cross-Reference) + Fase 3 (Memory)
   - Fundamentos: anÃ¡lisis transversal + memoria
   
2. **Semana 2**: Fase 2 (Artifacts UI)
   - RediseÃ±o UI + Sistema de renderizado
   
3. **Semana 3**: Fase 4 (Feedback) + Fase 5 (OCR bÃ¡sico)
   - ValidaciÃ³n + OCR sin Web Speech
   
4. **Semana 4**: Web Speech + IntegraciÃ³n completa
   - Diccionado + Refinamiento final

---

## ğŸ”„ ValidaciÃ³n por Fase

Cada fase incluye:
- âœ… Tests unitarios (en `tests/unit/`)
- âœ… Tests de integraciÃ³n
- âœ… Actualizar requirements.txt si necesario
- âœ… DocumentaciÃ³n en docstrings

---

## ğŸ“ Ejemplo de Flujo End-to-End (Post-ImplementaciÃ³n)

```
Usuario: "Compara el manual de red con la config actual"
    â†“
CrossReferenceAnalyzer.compare_documents()
    â†“
Sistema genera:
  - Tabla de diferencias (Artifact)
  - Recomendaciones (DecisionContext)
  - Guarda decisiÃ³n en memoria
    â†“
Usuario ve en panel derecho:
  - Informe interactivo
  - Checklist de cambios
  - Enlaces a fuentes
    â†“
Usuario: "Esta secciÃ³n necesita mÃ¡s detalle"
    â†“
FeedbackRefinement.process()
    â†“
Agent reescribe + explica cambio
    â†“
PrÃ³xima vez que pregunte about similar, agente recuerda preferencia
```

---

## ğŸ“Œ Notas Importantes

1. **No romper funcionalidad existente**: Todas las mejoras son aditivas
2. **Feature flags**: Cada componente puede desactivarse en settings.json
3. **Progresivo**: Fase pueden ejecutarse parcialmente
4. **Ollama dependency**: Asegurar disponibilidad de modelos
5. **UI/UX**: Considerar accesibilidad en artefactos

---

**Status**: Ready for Phase 1 Implementation
**Last Updated**: 2026-02-11

