# Docker Usage for Ollash Agent and Benchmark (External Ollama)

This document provides instructions on how to build and run the Ollash Agent and its benchmarking tool using Docker Compose, connecting to an **external Ollama instance**. This setup ensures an isolated environment for the agent while leveraging your existing Ollama server.

## Prerequisites

*   Docker Desktop (Windows/macOS) or Docker Engine (Linux) installed and running.
*   An **Ollama instance running** (default `http://localhost:11434`). Configure via the `OLLASH_OLLAMA_URL` environment variable if your Ollama runs on a different host.

## Setup

1.  **Clone the Repository (if you haven't already):**
    ```bash
    git clone <URL_DEL_REPOSITO>
    cd local-it-agent-ollash
    ```

2.  **Configure your Ollama server URL:**
    Copy the example environment file and set your Ollama server address:
    ```bash
    cp .env.example .env
    # Edit .env and set OLLASH_OLLAMA_URL to your Ollama server address
    # e.g., OLLASH_OLLAMA_URL=http://192.168.1.100:11434
    ```
    **Important:** Inside Docker, `localhost` refers to the container itself, not your host machine. You must set `OLLASH_OLLAMA_URL` to your machine's actual IP address or use `http://host.docker.internal:11434` (Docker Desktop only).

3.  **Build the Docker Image for the Agent:**
    Navigate to the root of your project where `Dockerfile` and `docker-compose.yml` are located.
    ```bash
    docker-compose build
    ```
    This command will build the `ollash` service image as defined in the `Dockerfile`.

## Ollama Models Configuration

The `ollash` agent expects your Ollama instance to have the following models already pulled:

*   **For Agent Operation (default and summary models):**
    *   `ministral-3:8b`
    *   `nemotron-3-nano:30b` (for benchmark summary)
*   **For Benchmarking (the models you want to test):**
    *   `devstral-small-2:latest`
    *   `gpt-oss:20b`
    *   `qwen3-coder:30b`

**If these models are not already present in your external Ollama instance, please pull them:**

```bash
ollama pull ministral-3:8b
ollama pull nemotron-3-nano:30b
ollama pull devstral-small-2:latest
ollama pull gpt-oss:20b
ollama pull qwen3-coder:30b
```

## Running the Ollash Agent

To run the agent in an isolated Docker container, connecting to your external Ollama:

### 1. Interactive Chat Mode

To start the agent in interactive chat mode:
```bash
docker-compose run --rm ollash python run_agent.py --chat
```
-   `--rm`: Automatically removes the container after it exits.
-   `ollash`: Specifies the service to run from `docker-compose.yml`.
-   `python run_agent.py --chat`: This is the command executed inside the `ollash` container.

### 2. Autonomous Project Generation (Automatic Mode)

To run the agent in automatic mode for autonomous project generation (you would typically provide a goal or project description via an input file or environment variable, depending on the agent's implementation for automatic mode):
```bash
# Example: Assuming your run_agent.py accepts a --goal argument for automatic mode
docker-compose run --rm ollash python run_agent.py --auto --goal "Create a simple Python Flask web application for a to-do list."
```
Replace `--auto --goal "..."` with the actual command-line arguments your agent requires for autonomous operation.

## Running the Benchmark

To run the benchmarking script in an isolated Docker container, connecting to your external Ollama:

### 1. Benchmark specific models (as specified by your current `settings.json` and requested models)

```bash
docker-compose run --rm ollash python benchmark.py --models devstral-small-2:latest gpt-oss:20b qwen3-coder:30b
```
This command will:
*   Start a temporary `ollash` container.
*   The `ollash` container will use the `config/settings.json` and the `OLLASH_OLLAMA_URL` environment variable to connect to your Ollama instance.
*   It will run `benchmark.py` testing `devstral-small-2:latest`, `gpt-oss:20b`, and `qwen3-coder:30b`.
*   The summary will be generated using `nemotron-3-nano:30b`.
*   The benchmark results (JSON file) will be saved in your host's project root directory.

### 2. Benchmark all models (if you want to test all models available on your external Ollama)

```bash
docker-compose run --rm ollash python benchmark.py
```

## Viewing Logs and Generated Projects

*   **Agent Logs:** Agent logs will be stored persistently in a Docker volume named `ollash_logs`.
    You can inspect them:
    ```bash
    docker volume inspect ollash_logs
    # Then navigate to the Mountpoint and view the log files (e.g., agent.log, benchmark_debug.log).
    ```
*   **Generated Projects:** Any projects generated by the autonomous agent will be stored in the `ollash_projects` Docker volume.
    ```bash
    docker volume inspect ollash_projects
    # Navigate to the Mountpoint to access generated project files.
    ```
*   **Benchmark Results:** The `benchmark_results_*.json` files generated by `benchmark.py` will be saved directly into your mounted project directory (`.:/app`), so you will find them in your host's project root folder.

## Stopping Services

To stop and remove the Docker containers and networks:
```bash
docker-compose down
```
To also remove the volumes (this will delete your agent logs and generated projects):
```bash
docker-compose down --volumes
```

This Docker setup provides a consistent and isolated environment for the Ollash Agent, leveraging your existing external Ollama instance.