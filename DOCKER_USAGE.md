# Docker Usage for Ollash Agent and Benchmark

This document provides instructions on how to build and run the Ollash Agent and its benchmarking tool using Docker and Docker Compose. This setup ensures an isolated environment for execution and simplifies dependency management.

## Prerequisites

*   Docker Desktop (Windows/macOS) or Docker Engine (Linux) installed and running.

## Setup

1.  **Clone the Repository (if you haven't already):**
    ```bash
    git clone <URL_DEL_REPOSITO>
    cd local-it-agent-ollash
    ```

2.  **Build the Docker Image for the Agent:**
    Navigate to the root of your project where `Dockerfile` and `docker-compose.yml` are located.
    ```bash
    docker-compose build
    ```
    This command will build the `moltbot` service image as defined in the `Dockerfile`.

## Running Ollama Models

The `docker-compose.yml` includes an `ollama` service. Before running the agent or benchmark, you need to pull the required models into the Ollama container.

1.  **Start the Ollama service:**
    ```bash
    docker-compose up -d ollama
    ```
    This will start the Ollama server in the background. You can check its status with `docker-compose ps`.

2.  **Pull the necessary models:**
    Connect to the running Ollama container and pull the models specified in `config/settings.json` or any other models you wish to use.
    The `moltbot` agent expects `ministral-3:8b` as its default and summary model, and `qwen3-coder-next` for code-related tasks.
    ```bash
    docker exec -it ollama_service ollama pull ministral-3:8b
    docker exec -it ollama_service ollama pull qwen3-coder-next
    # Pull any other models you want to benchmark or use.
    ```
    **Note:** This step is crucial. The agent and benchmark will fail if the models are not available to the Ollama service.

## Running the Ollash Agent

To run the agent in an isolated Docker container:

### 1. Interactive Chat Mode

To start the agent in interactive chat mode:
```bash
docker-compose run --rm moltbot python run_agent.py --chat
```
-   `--rm`: Automatically removes the container after it exits.
-   `moltbot`: Specifies the service to run from `docker-compose.yml`.
-   `python run_agent.py --chat`: This is the command executed inside the `moltbot` container.

### 2. Autonomous Project Generation (Automatic Mode)

To run the agent in automatic mode for autonomous project generation (you would typically provide a goal or project description via an input file or environment variable, depending on the agent's implementation for automatic mode):
```bash
# Example: Assuming your run_agent.py accepts a --goal argument for automatic mode
docker-compose run --rm moltbot python run_agent.py --auto --goal "Create a simple Python Flask web application for a to-do list."
```
Replace `--auto --goal "..."` with the actual command-line arguments your agent requires for autonomous operation.

## Running the Benchmark

To run the benchmarking script in an isolated Docker container:

### 1. Benchmark all local models

```bash
docker-compose run --rm moltbot python benchmark.py
```
This will benchmark all models available in your `ollama_service` that you have pulled.

### 2. Benchmark specific models

You can specify which models to benchmark using the `--models` argument:
```bash
docker-compose run --rm moltbot python benchmark.py --models ministral-3:8b qwen3-coder-next
```
Replace `ministral-3:8b qwen3-coder-next` with the space-separated names of the models you want to test.

## Viewing Logs and Generated Projects

*   **Agent Logs:** Agent logs will be stored persistently in a Docker volume named `moltbot_logs`.
    You can inspect them:
    ```bash
    docker volume inspect moltbot_logs
    # Then navigate to the Mountpoint and view the log files (e.g., agent.log, benchmark_debug.log).
    ```
*   **Generated Projects:** Any projects generated by the autonomous agent will be stored in the `moltbot_projects` Docker volume.
    ```bash
    docker volume inspect moltbot_projects
    # Navigate to the Mountpoint to access generated project files.
    ```
*   **Benchmark Results:** The `benchmark_results_*.json` files generated by `benchmark.py` will be saved directly into your mounted project directory (`.:/app`), so you will find them in your host's project root folder.

## Stopping Services

To stop and remove the Docker containers and networks (but keep the volumes):
```bash
docker-compose down
```
To stop and remove containers, networks, and volumes (this will delete your Ollama models, logs, and generated projects):
```bash
docker-compose down --volumes
```

This Docker setup provides a consistent and isolated environment, making it easier to reproduce results and manage dependencies for your Ollash Agent and its benchmarking activities.